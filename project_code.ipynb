{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "\t# load dataset\n",
    "\t(trainX_, trainY_), (testX, testY) = cifar10.load_data()\n",
    "\ttrainX = trainX_[:45000]\n",
    "\ttrainY = trainY_[:45000]\n",
    "\tvalX = trainX_[45000:]\n",
    "\tvalY = trainY_[45000:]\n",
    "\t# one hot encode target values\n",
    "\ttrainY = to_categorical(trainY)\n",
    "\tvalY = to_categorical(valY)\n",
    "\ttestY = to_categorical(testY)\n",
    "\treturn trainX, trainY, valX, valY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    np.random.seed(0)\n",
    "    val_idx = np.random.choice(range(trainX.shape[0]),5000,replace=False)\n",
    "    valX = trainX[val_idx]\n",
    "    valY = trainY[val_idx]\n",
    "\n",
    "    trainX = np.delete(trainX,val_idx,0)\n",
    "    trainY = np.delete(trainY,val_idx,0)\n",
    "\n",
    "    trainY = to_categorical(trainY)\n",
    "    valY = to_categorical(valY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, valX, valY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale pixels\n",
    "def prep_pixels(train, val, test, standardize_data):\n",
    "    # convert from integers to floats\n",
    "    train_norm = train.astype('float32')\n",
    "    val_norm = val.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # normalize to range 0-1\n",
    "    if standardize_data == True:\n",
    "        mean = np.mean(trainX, 0)\n",
    "        std = np.std(trainX, 0)\n",
    "        train_stand = (train_norm - mean) / std\n",
    "        val_stand = (val_norm - mean) / std\n",
    "        test_stand = (test_norm - mean) / std\n",
    "        return train_stand, val_stand, test_stand\n",
    "    else:\n",
    "        train_norm = train_norm / 255.0\n",
    "        val_norm = val_norm / 255.0\n",
    "        test_norm = test_norm / 255.0\n",
    "        return train_norm, val_norm, test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(reduce_data, standardize_data):\n",
    "    trainX, trainY, valX, valY, testX, testY = load_dataset()\n",
    "    trainX, valX, testX = prep_pixels(trainX, valX, testX, standardize_data)\n",
    "    if reduce_data==True:\n",
    "        return trainX[:10000], trainY[:10000], valX, valY, testX, testY\n",
    "    else:\n",
    "        return trainX, trainY, valX, valY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loadData() missing 1 required positional argument: 'standardize_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31704/204763077.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduce_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"trainX shape: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"trainY shape: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"valX shape: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"valY shape: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: loadData() missing 1 required positional argument: 'standardize_data'"
     ]
    }
   ],
   "source": [
    "trainX, trainY, valX, valY, testX, testY = loadData(reduce_data=True)\n",
    "print(\"trainX shape: \", trainX.shape)\n",
    "print(\"trainY shape: \", trainY.shape)\n",
    "print(\"valX shape: \", valX.shape)\n",
    "print(\"valY shape: \", valY.shape)\n",
    "print(\"testX shape: \", testX.shape)\n",
    "print(\"testY shape: \", testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(n_epochs, lr):\n",
    "    if n_epochs < 10:    \n",
    "        return lr\n",
    "    else:\n",
    "        return lr * math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(lr, optimizer, dropout_rate, lamda):\n",
    "\n",
    "\tif optimizer == SGD:\n",
    "\t\topt = SGD(lr, momentum = 0.9)\n",
    "\telif optimizer == Adam:\n",
    "\t\topt = Adam(lr)\n",
    "\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(lamda), input_shape=(32, 32, 3)))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(lamda)))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Dropout(dropout_rate))\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(lamda)))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(lamda)))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Dropout(dropout_rate))\n",
    "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(lamda)))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(lamda)))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Dropout(dropout_rate))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(lamda)))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(Dropout(dropout_rate))\n",
    "\tmodel.add(Dense(10, activation='softmax'))\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_diagnostics(history):\n",
    "\tplt.subplot(211)\n",
    "\tplt.title('Cross Entropy Loss')\n",
    "\tplt.plot(history.history['loss'], color='blue', label='train')\n",
    "\tplt.plot(history.history['val_loss'], color='orange', label='val')\n",
    "\tplt.subplot(212)\n",
    "\tplt.title('Classification Accuracy')\n",
    "\tplt.plot(history.history['accuracy'], color='blue', label='train')\n",
    "\tplt.plot(history.history['val_accuracy'], color='orange', label='val')\n",
    "\tfilename = sys.argv[0].split('/')[-1]\n",
    "\tplt.savefig(filename + '_plot.png')\n",
    "\tplt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runNetwork(n_epochs, n_batch, lr=0.001, optimizer=SGD, dropout_rate=0, lamda=0, augmentation=False, standardize_data=False, reduce_data=False):\n",
    "\ttrainX, trainY, valX, valY, testX, testY = loadData(reduce_data, standardize_data)\n",
    "\n",
    "\tif augmentation == True:\n",
    "\t\tmodel = define_model(lr, optimizer, dropout_rate, lamda)\n",
    "\t\tdatagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "\t\tit_train = datagen.flow(trainX, trainY, batch_size=n_batch)\n",
    "\t\tsteps = int(trainX.shape[0] / n_batch)\n",
    "\t\tcallback = LearningRateScheduler(scheduler)\n",
    "\t\thistory = model.fit(it_train, steps_per_epoch=steps, epochs=n_epochs, callbacks=[callback], validation_data=(valX, valY), verbose=1)\n",
    "\telse:\n",
    "\t\tmodel = define_model(lr, optimizer, dropout_rate, lamda)\n",
    "\t\thistory = model.fit(trainX, trainY, epochs=n_epochs, batch_size=n_batch, validation_data=(testX, testY), verbose=1)\n",
    "\n",
    "\t_, acc = model.evaluate(valX, valY, verbose=0)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\tsummarize_diagnostics(history)\n",
    "\n",
    "\treturn model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = runNetwork(n_epochs=1, n_batch=64, lr=.001, optimizer=Adam, dropout_rate=0.2, lamda=.005, augmentation=True, standardize_data=True, reduce_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6563df1e0326027644e1fb251e3f7c2b0f92920a4e6c3d0059086ac7c4851cbb"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('ML_course')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
