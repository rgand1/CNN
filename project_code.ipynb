{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_addons\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras.backend import clear_session\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import BatchNormalization, LayerNormalization\n",
    "from tensorflow_addons.layers import InstanceNormalization, GroupNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "\t# load dataset\n",
    "\t(trainX_, trainY_), (testX, testY) = cifar10.load_data()\n",
    "\ttrainX = trainX_[:45000]\n",
    "\ttrainY = trainY_[:45000]\n",
    "\tvalX = trainX_[45000:]\n",
    "\tvalY = trainY_[45000:]\n",
    "\t# one hot encode target values\n",
    "\ttrainY = to_categorical(trainY)\n",
    "\tvalY = to_categorical(valY)\n",
    "\ttestY = to_categorical(testY)\n",
    "\treturn trainX, trainY, valX, valY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    np.random.seed(0)\n",
    "    val_idx = np.random.choice(range(trainX.shape[0]),5000,replace=False)\n",
    "    valX = trainX[val_idx]\n",
    "    valY = trainY[val_idx]\n",
    "\n",
    "    trainX = np.delete(trainX,val_idx,0)\n",
    "    trainY = np.delete(trainY,val_idx,0)\n",
    "\n",
    "    trainY = to_categorical(trainY)\n",
    "    valY = to_categorical(valY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, valX, valY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale pixels\n",
    "def prep_pixels(train, val, test, standardize_data=False):\n",
    "    # convert from integers to floats\n",
    "    train_norm = train.astype('float32')\n",
    "    val_norm = val.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # normalize to range 0-1\n",
    "    if standardize_data == True:\n",
    "        mean = np.mean(trainX, 0)\n",
    "        std = np.std(trainX, 0)\n",
    "        train_stand = (train_norm - mean) / std\n",
    "        val_stand = (val_norm - mean) / std\n",
    "        test_stand = (test_norm - mean) / std\n",
    "        return train_stand, val_stand, test_stand\n",
    "    else:\n",
    "        train_norm = train_norm / 255.0\n",
    "        val_norm = val_norm / 255.0\n",
    "        test_norm = test_norm / 255.0\n",
    "        return train_norm, val_norm, test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(reduce_data=False, standardize_data=False):\n",
    "    trainX, trainY, valX, valY, testX, testY = load_dataset()\n",
    "    trainX, valX, testX = prep_pixels(trainX, valX, testX, standardize_data)\n",
    "    if reduce_data==True:\n",
    "        return trainX[:10000], trainY[:10000], valX, valY, testX, testY\n",
    "    else:\n",
    "        return trainX, trainY, valX, valY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX shape:  (10000, 32, 32, 3)\n",
      "trainY shape:  (10000, 10)\n",
      "valX shape:  (5000, 32, 32, 3)\n",
      "valY shape:  (5000, 10)\n",
      "testX shape:  (10000, 32, 32, 3)\n",
      "testY shape:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY, valX, valY, testX, testY = loadData(reduce_data=True)\n",
    "print(\"trainX shape: \", trainX.shape)\n",
    "print(\"trainY shape: \", trainY.shape)\n",
    "print(\"valX shape: \", valX.shape)\n",
    "print(\"valY shape: \", valY.shape)\n",
    "print(\"testX shape: \", testX.shape)\n",
    "print(\"testY shape: \", testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(n_epochs, lr):\n",
    "    if n_epochs < 10:    \n",
    "        return lr\n",
    "    else:\n",
    "        return lr * math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(lr, optimizer, dropout_rate, lamda, normalization=\"batch\"):\n",
    "\n",
    "\tclear_session()\n",
    "\n",
    "\tif optimizer == SGD:\n",
    "\t\topt = SGD(lr, momentum = 0.9)\n",
    "\telif optimizer == Adam:\n",
    "\t\topt = Adam(lr)\n",
    "\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(lamda), input_shape=(32, 32, 3)))\n",
    "\tif normalization == \"batch\":\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "\telif normalization == \"layer\":\n",
    "\t\tmodel.add(LayerNormalization())\n",
    "\telif normalization == \"group\":\n",
    "\t\tmodel.add(GroupNormalization())\n",
    "\telif normalization == \"instance\":\n",
    "\t\tmodel.add(InstanceNormalization())\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(lamda)))\n",
    "\tif normalization == \"batch\":\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "\telif normalization == \"layer\":\n",
    "\t\tmodel.add(LayerNormalization())\n",
    "\telif normalization == \"group\":\n",
    "\t\tmodel.add(GroupNormalization())\n",
    "\telif normalization == \"instance\":\n",
    "\t\tmodel.add(InstanceNormalization())\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Dropout(dropout_rate))\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(lamda)))\n",
    "\tif normalization == \"batch\":\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "\telif normalization == \"layer\":\n",
    "\t\tmodel.add(LayerNormalization())\n",
    "\telif normalization == \"group\":\n",
    "\t\tmodel.add(GroupNormalization())\n",
    "\telif normalization == \"instance\":\n",
    "\t\tmodel.add(InstanceNormalization())\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(lamda)))\n",
    "\tif normalization == \"batch\":\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "\telif normalization == \"layer\":\n",
    "\t\tmodel.add(LayerNormalization())\n",
    "\telif normalization == \"group\":\n",
    "\t\tmodel.add(GroupNormalization())\n",
    "\telif normalization == \"instance\":\n",
    "\t\tmodel.add(InstanceNormalization())\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Dropout(dropout_rate))\n",
    "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(lamda)))\n",
    "\tif normalization == \"batch\":\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "\telif normalization == \"layer\":\n",
    "\t\tmodel.add(LayerNormalization())\n",
    "\telif normalization == \"group\":\n",
    "\t\tmodel.add(GroupNormalization())\n",
    "\telif normalization == \"instance\":\n",
    "\t\tmodel.add(InstanceNormalization())\n",
    "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(lamda)))\n",
    "\tif normalization == \"batch\":\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "\telif normalization == \"layer\":\n",
    "\t\tmodel.add(LayerNormalization())\n",
    "\telif normalization == \"group\":\n",
    "\t\tmodel.add(GroupNormalization())\n",
    "\telif normalization == \"instance\":\n",
    "\t\tmodel.add(InstanceNormalization())\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Dropout(dropout_rate))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(lamda)))\n",
    "\tif normalization == \"batch\":\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "\telif normalization == \"layer\":\n",
    "\t\tmodel.add(LayerNormalization())\n",
    "\telif normalization == \"group\":\n",
    "\t\tmodel.add(GroupNormalization())\n",
    "\telif normalization == \"instance\":\n",
    "\t\tmodel.add(InstanceNormalization())\n",
    "\tmodel.add(Dropout(dropout_rate))\n",
    "\tmodel.add(Dense(10, activation='softmax'))\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_diagnostics(history, var):\n",
    "\n",
    "\tplt.subplot(211)\n",
    "\tplt.title('Cross Entropy Loss')\n",
    "\tplt.plot(history.history['loss'], color='blue', label='train')\n",
    "\tplt.plot(history.history['val_loss'], color='orange', label='val')\n",
    "\tplt.subplot(212)\n",
    "\tplt.title(f'Classification Accuracy: {max(history.history[\"val_accuracy\"])*100:.2f}%')\n",
    "\tplt.plot(history.history['accuracy'], color='blue', label='train')\n",
    "\tplt.plot(history.history['val_accuracy'], color='orange', label='val')\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(os.path.join(root_path, 'Plots_jan', f'10 Epochs, variation={var}, Accuracy={max(history.history[\"val_accuracy\"])*100:.2f}%.png'))\n",
    "\t#plt.show()\n",
    "\tplt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runNetwork(n_epochs, n_batch, lr=0.001, optimizer=SGD, dropout_rate=0, lamda=0, normalization=None, datagenerator=None, augmentation=False, standardize_data=False, reduce_data=False):\n",
    "\ttrainX, trainY, valX, valY, testX, testY = loadData(reduce_data, standardize_data)\n",
    "\n",
    "\tif augmentation == True:\n",
    "\t\tmodel = define_model(lr, optimizer, dropout_rate, lamda, normalization)\n",
    "\t\tif datagenerator==None:\n",
    "\t\t\tdatagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True, vertical_flip=True)\n",
    "\t\telse:\n",
    "\t\t\tdatagen = datagenerator\n",
    "\t\tit_train = datagen.flow(trainX, trainY, batch_size=n_batch)\n",
    "\t\tsteps = int(trainX.shape[0] / n_batch)\n",
    "\t\tcallback = LearningRateScheduler(scheduler)\n",
    "\t\thistory = model.fit(it_train, steps_per_epoch=steps, epochs=n_epochs, callbacks=[callback], validation_data=(valX, valY), verbose=1)\n",
    "\telse:\n",
    "\t\tmodel = define_model(lr, optimizer, dropout_rate, lamda, normalization)\n",
    "\t\tcallback = LearningRateScheduler(scheduler)\n",
    "\t\thistory = model.fit(trainX, trainY, epochs=n_epochs, batch_size=n_batch, callbacks=[callback], validation_data=(testX, testY), verbose=1)\n",
    "\n",
    "\t_, acc = model.evaluate(valX, valY, verbose=0)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t#summarize_diagnostics(history, normalization)\n",
    "\n",
    "\treturn model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = runNetwork(n_epochs=1, n_batch=64, lr=.001, optimizer=Adam, dropout_rate=0.2, lamda=.005, normalization=\"instance\", augmentation=True, standardize_data=True, reduce_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = [\"batch\", \"layer\", \"group\", \"instance\"]\n",
    "Historys_norm = []\n",
    "\n",
    "for norm in norms:\n",
    "    model, history = runNetwork(n_epochs=10, n_batch=64, lr=.001, optimizer=Adam, dropout_rate=0.2, lamda=.005, normalization=norm, augmentation=True, standardize_data=True, reduce_data=True)\n",
    "    Historys_norm.append(history)\n",
    "    summarize_diagnostics(history, norm)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen1 = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True, vertical_flip=True)\n",
    "datagen2 = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True, rotation_range=90)\n",
    "datagen3 = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True, zoom_range=1.5)\n",
    "datagen4 = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True, channel_shift_range=4.5)\n",
    "\n",
    "datagens = [datagen1, datagen2, datagen3, datagen4]\n",
    "Historys = []\n",
    "\n",
    "for datagen in datagens:\n",
    "    model, history = runNetwork(n_epochs=10, n_batch=64, lr=.001, optimizer=Adam, dropout_rate=0.2, lamda=.005, normalization=\"batch\", datagenerator=datagen, augmentation=True, standardize_data=True, reduce_data=True)\n",
    "    Historys.append(history)\n",
    "    summarize_diagnostics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = [0.1, 0.2, 0.3, 0.4]\n",
    "Historys_drop = []\n",
    "\n",
    "for drop in drops:\n",
    "    model, history = runNetwork(n_epochs=10, n_batch=64, lr=.001, optimizer=Adam, dropout_rate=drop, lamda=.005, normalization=\"batch\", augmentation=True, standardize_data=True, reduce_data=True)\n",
    "    Historys_drop.append(history)\n",
    "    summarize_diagnostics(history, drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install tensorflow-gpu==2.0.0-alpha0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "409590e0cdb5f6307d1cfad9f1f69001bad2d99124756299e636d7e82d23f5e6"
  },
  "kernelspec": {
   "display_name": "Python 3.5.6 ('DL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
